<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhouzhou🥣️</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-07-15T16:56:15.071Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Zhuhao Zhou</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Differentiable Scene Graphs 论文笔记</title>
    <link href="http://example.com/2022/07/15/DSG-note/"/>
    <id>http://example.com/2022/07/15/DSG-note/</id>
    <published>2022-07-15T09:39:39.000Z</published>
    <updated>2022-07-15T16:56:15.071Z</updated>
    
    <content type="html"><![CDATA[<h3 id="可微场景图原论文连接-httpsarxiv.orgabs1902.10200">可微场景图原论文连接 <a href="https://arxiv.org/abs/1902.10200" class="uri">https://arxiv.org/abs/1902.10200</a></h3><h2>abstract</h2><table><tbody><tr class="odd"><td>复杂视觉场景的推理涉及对实体及其关系的感知。场景图(SGs)通过给实体(node)和关系(edge)分配标签，为推理任务提供了一种自然的表示。基于SGs的推理系统的训练通常分为两个步骤:首先，一个模型被训练来根据图像预测场景图，然后一个单独的模型被训练来根据预测的场景图进行推理。然而，以端到端方式训练这种系统似乎更可取。我们要解决的挑战是：场景图表示是不可微的，因此，还不知道怎么使用场景图作为中间组件。本论文提出可微场景图(DSGs)，一种可微端到端优化的图像表示，只需要从下游任务的监督。DSGs为所有区域和区域对提供密集表示，并且不花费建模能力在图像中不包含对象或相关关系的区域。我们评估了在三个基准数据集中识别参考关系（RR）的挑战性任务的模型：Visual Genome，VRD和CLEVR。将DSG作为中间表示会带来最前沿的表现。完整代码可在<a href="https://github.com/shikorab/dsg" class="uri">https://github.com/shikorab/dsg</a>上找到。</td></tr><tr class="even"><td>## introduction</td></tr></tbody></table><p>理解丰富的视觉场景的完整语义是一项复杂的任务，包括检测单个实体，以及对实体的组合和它们之间的关系进行推理。为了共同表示实体和它们的关系，很自然地将它们视为一个图，其中节点是实体，边代表关系。这种表示通常被称为场景图(SGs)。由于SGs可以直接地对图像进行推理，因此已作出大量努力从原始图像中推断出它们。虽然场景图已被证明在各种任务中都是有用的，但将它们作为视觉推理系统的组成部分是具有挑战性的。(a)由于场景图是离散的表示，所以很难从下游任务中以端到端的方式学习它们。b)另一种方法是将SG预测器与监督数据分开进行预训练，但这需要费力且难以负担的人工注释。此外，预先训练的SG预测器有较差的收敛性，因为他们经过预先培训的标签很少适合下游任务的需求。例如，考虑到游行的图像和“指着骑在黑马上的军官”的问题，那匹马可能不是图中的节点，而“军官”一词可能不在词汇中。鉴于这些局限性，如何使场景图在视觉推理应用中发挥作用是一个有待解决的问题。在这项工作中，我们描述了可微场景图(DSG)，它解决了上述的挑战(图1)。</p><center><img data-src="figure1_iccv.jpg" width="50%"> <br><div style="color:orange;     display: inline-block;    color: #999;    padding: 2px;">图1.可微场景图:一种类似图的中间表示，为图像中的每个实体和实体对提供分布式表示。可微场景图可以用 梯度下降法以端到端的方式学习，只需要对下游的视觉推理任务进行监督(referring relationship)。</div></center><p>DSG是一种从下游推理任务的监督端到端训练的中间表示。其核心思想是放宽场景图的离散性，使每个实体和关系都用稠密可微的描述子来描述。我们展示了DSGs在解决参考关系(RR)任务中的好处(see Figure 1). 在这里，给定图像和triplet query&lt;subject，relation，object&gt;, 模型需要找到关系中subject和object的矩形框（bounding boxes）。我们用DSG作为中间组件训练RR模型。因此，DSG在训练时并没有对实体和关系进行直接监督，而是对下游RR任务使用多个监督信号。我们在三个标准RR数据集上评估了我们的方法:Vi-sual Genome、VRD和CLEVR，发现与最先进的方法相比，DSGs显著提高了性能。</p><p>综上所述，论文中的新贡献是:(1)一种新的可微分场景图表示方法，用于视觉推理，它能捕捉图像中多个实体及其关系的信息。我们描述了如何用一个下游的视觉推理任务来端到端训练DSG，而无需手动注释的场景图的直接监督。(2)以DSG为核心组件，提出了一种新的结构用于referring relationship任务。(3) Visual Genome、VRD和CLEVR数据集上referring relationship任务的最新研究结果。</p><h2 id="reffering-relationship-the-learning-setup">## Reffering Relationship: The learning Setup</h2><p>在引用关系任务中，我们得到一个图像<span class="math inline">\(I\)</span>和一个主题-关系-对象查询<span class="math inline">\(q=&lt;s,r,o&gt;\)</span>.目标是为主题输出一个边界框<span class="math inline">\(B_{s}\)</span>，为对象输出另一个边界框<span class="math inline">\(B_{o}\)</span>。在实践中，有时每一个都有几个边界框。图1给出了一个示例查询和预期输出。</p><p>根据“Referring Relationships” 论文，我们专注于用标注数据训练一个referring relationship。也就是说，我们使用一个由图像、查询和这些查询的正确方框组成的训练集。用$ {(I_{j}, q_{j}, (B_{j}^{S}, B_{j}^{o}))}_{j}^{N} $</p>]]></content>
    
    
    <summary type="html">可微场景图论文阅读笔记</summary>
    
    
    
    <category term="论文笔记" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="SGG" scheme="http://example.com/tags/SGG/"/>
    
  </entry>
  
</feed>
